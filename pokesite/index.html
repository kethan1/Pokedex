<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Audio to ONNX Inference</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
<h2>Upload Audio File</h2>
<input type="file" id="audioInput" accept="audio/*" />
<pre id="output"></pre>

<script>
    const audioInput = document.getElementById("audioInput");
    const output = document.getElementById("output");

    function softmax(arr) {
        const max = Math.max(...arr);
        const exps = arr.map(x => Math.exp(x - max));
        const sum = exps.reduce((a, b) => a + b);
        return exps.map(x => x / sum);
    }

    async function runONNXModel(logMelSpec) {
        try {
            // Check the shape and values
            console.log("Input spectrogram shape:", logMelSpec.shape);
            console.log("Input spectrogram stats:", {
                min: tf.min(logMelSpec).dataSync()[0],
                max: tf.max(logMelSpec).dataSync()[0],
                mean: tf.mean(logMelSpec).dataSync()[0]
            });

            const transposed = logMelSpec.transpose(); // [64, frames]
            const inputTensor = transposed.expandDims(0).expandDims(0); // [1, 1, 64, frames]

            console.log("Final input tensor shape:", inputTensor.shape);

            const session = await ort.InferenceSession.create("/model.onnx");
            const inputName = session.inputNames[0];
            const tensor = new ort.Tensor("float32", inputTensor.dataSync(), inputTensor.shape);

            const results = await session.run({ [inputName]: tensor });
            const outputName = session.outputNames[0];
            const outputData = results[outputName].data;

            console.log("Raw model output:", Array.from(outputData));

            const probs = softmax(Array.from(outputData));
            console.log("Softmax probabilities:", probs);

            const maxIdx = probs.indexOf(Math.max(...probs));

            const labels = [
                "squirtle",
                "psyduck",
                "pikachu",
                "mew",
                "magikarp",
                "gengar",
                "charmander",
                "bulbasaur",
            ];

            const predictedLabel = labels[maxIdx] ?? "Unknown";
            output.innerText += `\n\nPrediction: ${predictedLabel} (Confidence: ${(probs[maxIdx] * 100).toFixed(2)}%)`;

            // Show all probabilities for debugging
            output.innerText += `\n\nAll probabilities:`;
            labels.forEach((label, idx) => {
                output.innerText += `\n${label}: ${(probs[idx] * 100).toFixed(2)}%`;
            });

        } catch (error) {
            console.error("ONNX inference error:", error);
            output.innerText += `\nError during inference: ${error.message}`;
        }
    }

    async function resampleAudio(audioData, originalSampleRate, targetSampleRate) {
        if (originalSampleRate === targetSampleRate) {
            return audioData;
        }

        const ratio = targetSampleRate / originalSampleRate;
        const newLength = Math.round(audioData.length * ratio);
        const resampled = new Float32Array(newLength);

        for (let i = 0; i < newLength; i++) {
            const srcIndex = i / ratio;
            const srcIndexFloor = Math.floor(srcIndex);
            const srcIndexCeil = Math.min(srcIndexFloor + 1, audioData.length - 1);
            const fraction = srcIndex - srcIndexFloor;

            resampled[i] = audioData[srcIndexFloor] * (1 - fraction) +
                audioData[srcIndexCeil] * fraction;
        }

        return resampled;
    }

    function normalizeAudio(audioData) {
        const max = Math.max(...audioData.map(Math.abs));
        if (max === 0) return audioData;
        return audioData.map(x => x / max);
    }

    function amplitudeToDb(tensor, amin = 1e-10, topDb = 80.0) {
        // Convert amplitude spectrogram to dB scale
        // amin: minimum threshold for amplitude
        // topDb: maximum dB range

        const clampedTensor = tf.maximum(tensor, amin);
        const dbTensor = tf.mul(tf.log(clampedTensor), 10.0 / Math.log(10)); // 10 * log10(x)

        // Apply top_db clipping
        const maxDb = tf.max(dbTensor);
        const minDb = tf.sub(maxDb, topDb);

        return tf.maximum(dbTensor, minDb);
    }

    async function createMelSpectrogram(audioData, sr = 16000) {
        // Normalize audio
        const normalizedAudio = normalizeAudio(audioData);
        const waveform = tf.tensor1d(normalizedAudio);

        // Parameters - these might need adjustment based on your model training
        const frameLength = 400;  // Increased from 400
        const frameStep = 200;    // Increased from 200
        const fftLength = 400;    // Increased from 400
        const numMels = 64;

        console.log("Audio preprocessing params:", {frameLength, frameStep, fftLength, numMels});
        console.log("Input audio length:", audioData.length, "samples");

        const frames = tf.signal.frame(waveform, frameLength, frameStep);
        console.log("Frames shape:", frames.shape);

        const windowed = frames.mul(tf.signal.hannWindow(frameLength));
        const fft = tf.spectral.rfft(windowed, fftLength);
        const magnitude = tf.abs(fft);

        // Add small epsilon before taking power
        const powerSpec = tf.square(magnitude).add(1e-10);

        function createMelFilterbank(numMels, fftSize, sampleRate) {
            const nyquist = sampleRate / 2;
            const melMin = 0;
            const melMax = 2595 * Math.log10(1 + nyquist / 700); // Use proper mel formula

            const melPoints = [];
            for (let i = 0; i <= numMels + 1; i++) {
                melPoints.push(melMin + (melMax - melMin) * i / (numMels + 1));
            }

            const hzPoints = melPoints.map(mel => 700 * (Math.pow(10, mel / 2595) - 1));
            const binPoints = hzPoints.map(hz => Math.floor((fftSize + 1) * hz / nyquist));

            const filters = [];
            for (let m = 1; m <= numMels; m++) {
                const lower = binPoints[m - 1];
                const center = binPoints[m];
                const upper = binPoints[m + 1];
                const filter = [];

                for (let k = 0; k < Math.floor(fftSize / 2) + 1; k++) {
                    if (k < lower || k > upper) {
                        filter.push(0);
                    } else if (k <= center) {
                        filter.push((k - lower) / (center - lower));
                    } else {
                        filter.push((upper - k) / (upper - center));
                    }
                }
                filters.push(filter);
            }
            return tf.tensor2d(filters);
        }

        const melFilterbank = createMelFilterbank(numMels, fftLength, sr);
        const melSpec = tf.matMul(powerSpec, melFilterbank.transpose());

        console.log("Mel spectrogram shape before padding:", melSpec.shape);

        // Pad or trim spectrogram to 200 frames
        let paddedSpec = melSpec;
        const currentFrames = melSpec.shape[0];

        if (currentFrames < 200) {
            const padAmount = 200 - currentFrames;
            const pad = tf.zeros([padAmount, melSpec.shape[1]]);
            paddedSpec = tf.concat([melSpec, pad], 0);
        } else if (currentFrames > 200) {
            paddedSpec = melSpec.slice([0, 0], [200, melSpec.shape[1]]);
        }

        // Apply log with proper epsilon
        const logMelSpec = tf.log(paddedSpec.add(1e-6));

        // Apply amplitude to dB conversion
        const dbMelSpec = amplitudeToDb(logMelSpec);

        // Optional: Apply mean normalization per feature
        const mean = tf.mean(dbMelSpec, 0, true);
        const std = tf.moments(dbMelSpec, 0, true).variance.sqrt().add(1e-8);
        const normalizedSpec = dbMelSpec.sub(mean).div(std);

        console.log("Final spectrogram shape:", normalizedSpec.shape);
        console.log("Final spectrogram stats:", {
            min: tf.min(normalizedSpec).dataSync()[0],
            max: tf.max(normalizedSpec).dataSync()[0],
            mean: tf.mean(normalizedSpec).dataSync()[0]
        });

        return normalizedSpec;
    }

    audioInput.onchange = async (e) => {
        const file = e.target.files[0];
        if (!file) return;

        output.innerText = "Processing audio...";

        try {
            const arrayBuffer = await file.arrayBuffer();

            // Create audio context without forcing sample rate first
            const audioCtx = new AudioContext();
            const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

            console.log("Original audio info:", {
                duration: audioBuffer.duration,
                sampleRate: audioBuffer.sampleRate,
                channels: audioBuffer.numberOfChannels,
                length: audioBuffer.length
            });

            // Get mono audio data
            let audioData = audioBuffer.getChannelData(0);

            // Resample to 16kHz if needed
            if (audioBuffer.sampleRate !== 16000) {
                console.log(`Resampling from ${audioBuffer.sampleRate}Hz to 16000Hz`);
                audioData = await resampleAudio(audioData, audioBuffer.sampleRate, 16000);
            }

            output.innerText = `Processing audio: ${audioData.length} samples at 16kHz\n`;
            output.innerText += `Duration: ${(audioData.length / 16000).toFixed(2)} seconds\n`;

            const logMelSpec = await createMelSpectrogram(audioData, 16000);
            output.innerText += `Mel spectrogram shape: ${logMelSpec.shape}\n`;

            await runONNXModel(logMelSpec);

        } catch (error) {
            console.error("Processing error:", error);
            output.innerText += `\nError: ${error.message}`;
        }
    };
</script>
</body>
</html>