<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Audio to ONNX Inference</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
<h2>Upload Audio File</h2>
<input type="file" id="audioInput" accept="audio/*" />
<pre id="output"></pre>

<script>
    const audioInput = document.getElementById("audioInput");
    const output = document.getElementById("output");

    function softmax(arr) {
        const max = Math.max(...arr);
        const exps = arr.map(x => Math.exp(x - max));
        const sum = exps.reduce((a, b) => a + b);
        return exps.map(x => x / sum);
    }

    async function runONNXModel(logMelSpec) {
        const transposed = logMelSpec.transpose(); // [64, frames]
        const inputTensor = transposed.expandDims(0).expandDims(0); // [1, 1, 64, frames]
        const inputData = inputTensor.dataSync();

        const session = await ort.InferenceSession.create("/model.onnx");
        const inputName = session.inputNames[0];
        const tensor = new ort.Tensor("float32", inputData, inputTensor.shape);

        const results = await session.run({ [inputName]: tensor });
        const outputName = session.outputNames[0];
        const outputData = results[outputName].data;

        const probs = softmax(Array.from(outputData));
        const maxIdx = probs.indexOf(Math.max(...probs));

        const labels = [
            "squirtle",
            "psyduck",
            "pikachu",
            "mew",
            "magikarp",
            "gengar",
            "charmander",
            "bulbasaur",
        ];
        console.log("Raw model output:", outputData);

        const predictedLabel = labels[maxIdx] ?? "Unknown";
        output.innerText += `\n\nPrediction: ${predictedLabel} (Confidence: ${(probs[maxIdx] * 100).toFixed(2)}%)`;
    }

    async function createMelSpectrogram(audioData, sr = 16000) {
        const waveform = tf.tensor1d(audioData);
        const frameLength = 400, frameStep = 200, fftLength = 400, numMels = 64;

        const frames = tf.signal.frame(waveform, frameLength, frameStep);
        const windowed = frames.mul(tf.signal.hannWindow(frameLength));
        const fft = tf.spectral.rfft(windowed, fftLength);
        const mag = tf.abs(fft);

        function createMelFilterbank(numMels, fftSize, sampleRate) {
            const melMin = 0;
            const melMax = 1127 * Math.log(1 + sampleRate / 2 / 700);
            const melPoints = tf.linspace(melMin, melMax, numMels + 2);
            const hzPoints = melPoints.mul(700).exp().sub(1);
            const bin = hzPoints.mul(fftSize / 2 + 1).div(sampleRate / 2).floor();

            const filters = [];
            const binArray = bin.arraySync();

            for (let m = 1; m <= numMels; m++) {
                const lower = binArray[m - 1];
                const center = binArray[m];
                const upper = binArray[m + 1];
                const filter = [];
                for (let k = 0; k < fftSize / 2 + 1; k++) {
                    if (k < lower) filter.push(0);
                    else if (k < center) filter.push((k - lower) / (center - lower));
                    else if (k < upper) filter.push((upper - k) / (upper - center));
                    else filter.push(0);
                }
                filters.push(filter);
            }
            return tf.tensor2d(filters);
        }

        const melFilterbank = createMelFilterbank(numMels, fftLength, sr);
        const melSpec = tf.matMul(mag, melFilterbank.transpose());

        // Pad or trim spectrogram to 200 frames
        let paddedSpec = melSpec;
        const currentFrames = melSpec.shape[0];
        if (currentFrames < 200) {
            const padAmount = 200 - currentFrames;
            const pad = tf.zeros([padAmount, melSpec.shape[1]]);
            paddedSpec = melSpec.concat(pad, 0);
        } else if (currentFrames > 200) {
            paddedSpec = melSpec.slice(0, 200);
        }

        return tf.log(paddedSpec.add(1e-6));
    }


    audioInput.onchange = async (e) => {
        const file = e.target.files[0];
        if (!file) return;
        const arrayBuffer = await file.arrayBuffer();
        const audioCtx = new AudioContext({ sampleRate: 16000 });
        const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
        const audioData = audioBuffer.getChannelData(0); // mono

        const logMelSpec = await createMelSpectrogram(audioData, 16000);
        output.innerText = `Mel spectrogram shape: ${logMelSpec.shape}`;

        await runONNXModel(logMelSpec);
    };
</script>
</body>
</html>
